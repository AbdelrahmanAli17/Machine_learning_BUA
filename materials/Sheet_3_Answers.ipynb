{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Task: Load the Breast Cancer dataset and create a basic logistic regression model without preprocessing."
      ],
      "metadata": {
        "id": "HGOYBSv-f4AG"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "F3oTEjI_fzWq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ee690764-2464-483c-8a34-a53a0f234efc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.9561\n",
            "Confusion Matrix:\n",
            "[[39  4]\n",
            " [ 1 70]]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:465: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
            "STOP: TOTAL NO. OF ITERATIONS REACHED LIMIT.\n",
            "\n",
            "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
            "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
            "Please also refer to the documentation for alternative solver options:\n",
            "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
            "  n_iter_i = _check_optimize_result(\n"
          ]
        }
      ],
      "source": [
        "# Import necessary libraries\n",
        "from sklearn.datasets import load_breast_cancer\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import confusion_matrix, accuracy_score\n",
        "import pandas as pd\n",
        "\n",
        "# Load the dataset\n",
        "data = load_breast_cancer()\n",
        "X = data.data  # Features\n",
        "y = data.target  # Target (0 = malignant, 1 = benign)\n",
        "\n",
        "# Split the data\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create and train logistic regression model\n",
        "model = LogisticRegression(random_state=42)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Make predictions\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "cm = confusion_matrix(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### Explanation:\n",
        "- load_breast_cancer() loads a binary classification dataset\n",
        "- X contains 30 features about tumor characteristics\n",
        "- y contains binary labels (0 = malignant, 1 = benign)\n",
        "- LogisticRegression is used for binary classification\n",
        "- confusion_matrix shows:\n",
        "\n",
        "    [True Negative, False Positive]\n",
        "\n",
        "    [False Negative, True Positive]\n",
        "\n"
      ],
      "metadata": {
        "id": "FqZ0bWM3gE83"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Notes:\n",
        "1- Logistic Regression is for classification, Linear Regression for regression\n",
        "\n",
        "2- Outputs probabilities between 0 and 1 using sigmoid function\n",
        "\n",
        "3- Perfect for binary classification problems\n",
        "\n",
        "4- Interpretable and fast for small to medium datasets\n",
        "\n",
        "5- Requires feature scaling for best performance\n",
        "\n",
        "6- Can be extended to multi-class problems"
      ],
      "metadata": {
        "id": "jysTiqiOE56L"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task: Calculate and interpret precision, recall, and F1-score from the confusion matrix."
      ],
      "metadata": {
        "id": "BpHatCrfgFSK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# Calculate evaluation metrics\n",
        "tn, fp, fn, tp = cm.ravel()\n",
        "\n",
        "precision = tp / (tp + fp)\n",
        "recall = tp / (tp + fn)\n",
        "f1 = 2 * (precision * recall) / (precision + recall)\n",
        "\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1-Score: {f1:.4f}\")\n",
        "\n",
        "# Using sklearn's built-in function\n",
        "print(\"\\nDetailed Classification Report:\")\n",
        "print(classification_report(y_test, y_pred, target_names=['Malignant', 'Benign']))"
      ],
      "metadata": {
        "id": "m9ATeAZpgFvf",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "57515f18-3c9e-4d10-ace8-a5bf0df5e768"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Precision: 0.9459\n",
            "Recall: 0.9859\n",
            "F1-Score: 0.9655\n",
            "\n",
            "Detailed Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "   Malignant       0.97      0.91      0.94        43\n",
            "      Benign       0.95      0.99      0.97        71\n",
            "\n",
            "    accuracy                           0.96       114\n",
            "   macro avg       0.96      0.95      0.95       114\n",
            "weighted avg       0.96      0.96      0.96       114\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "- Precision: How many predicted positives are actually positive\n",
        "- Recall: How many actual positives are correctly predicted\n",
        "- F1-Score: Harmonic mean of precision and recall\n",
        "- Macro Average: Simple average of both classes (0.97 + 0.97) / 2 = 0.97\n",
        "\n",
        "- Weighted Average: Average weighted by support (43×0.97 + 71×0.97) / 114 = 0.97\n",
        "\n",
        "Interpretation:\n",
        "- High precision: Few false alarms\n",
        "- High recall: Misses few positive cases\n",
        "- Good model should have both high precision and recall\n"
      ],
      "metadata": {
        "id": "HtmbP-mEgGMP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Key Takeaways\n",
        "\n",
        "\n",
        "1.   Precision: Quality of positive predictions → \"How trustworthy are our 'yes' answers?\"\n",
        "\n",
        "2. Recall: Coverage of actual positives → \"How many actual 'yes' cases did we find?\"\n",
        "\n",
        "3. F1-Score: Balanced measure when both matter equally\n",
        "\n",
        "4. Context Matters: In healthcare, recall is often more important than precision\n",
        "\n",
        "5. Trade-offs: Improving one metric often worsens another\n",
        "\n",
        "\n",
        "For the breast cancer model(medical in general):\n",
        "\n",
        "  1. High recall means you're good at detecting actual cancer cases (safety first!)\n",
        "\n",
        "  2. High precision means you're not causing unnecessary worry with false alarms\n",
        "\n",
        "  3. Your balanced F1-score suggests good overall performance for medical use"
      ],
      "metadata": {
        "id": "OSlTB2KZJ4fo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task: Apply logistic regression to classify three types of iris flowers."
      ],
      "metadata": {
        "id": "_NLiO600gGPM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.datasets import load_iris\n",
        "\n",
        "# Load iris dataset\n",
        "iris = load_iris()\n",
        "X_iris = iris.data\n",
        "y_iris = iris.target\n",
        "\n",
        "# Split the data\n",
        "X_train_iris, X_test_iris, y_train_iris, y_test_iris = train_test_split(\n",
        "    X_iris, y_iris, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Create multi-class logistic regression model\n",
        "# multi_class='multinomial' enables Softmax for multi-class classification\n",
        "model_iris = LogisticRegression(multi_class='multinomial', random_state=42)\n",
        "model_iris.fit(X_train_iris, y_train_iris)\n",
        "\n",
        "# Predictions\n",
        "y_pred_iris = model_iris.predict(X_test_iris)\n",
        "\n",
        "# Evaluate\n",
        "accuracy_iris = accuracy_score(y_test_iris, y_pred_iris)\n",
        "cm_iris = confusion_matrix(y_test_iris, y_pred_iris)\n",
        "\n",
        "print(f\"Iris Dataset Accuracy: {accuracy_iris:.4f}\")\n",
        "print(\"Iris Confusion Matrix:\")\n",
        "print(cm_iris)\n",
        "print(f\"Class names: {iris.target_names}\")"
      ],
      "metadata": {
        "id": "1pYZTgP9gGbF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "03dfb32a-6e39-4855-b9f4-087b98c7e4fb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/sklearn/linear_model/_logistic.py:1247: FutureWarning: 'multi_class' was deprecated in version 1.5 and will be removed in 1.7. From then on, it will always use 'multinomial'. Leave it to its default value to avoid this warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Iris Dataset Accuracy: 1.0000\n",
            "Iris Confusion Matrix:\n",
            "[[19  0  0]\n",
            " [ 0 13  0]\n",
            " [ 0  0 13]]\n",
            "Class names: ['setosa' 'versicolor' 'virginica']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "##Multi-class Classification:\n",
        "\n",
        "- Softmax function generalizes sigmoid for multiple classes\n",
        "\n",
        "  Step 1: exponentiate all scores → [e², e¹, e⁰·¹] = [7.39, 2.72, 1.11]\n",
        "\n",
        "  Step 2: sum all exponentials → 7.39 + 2.72 + 1.11 = 11.22\n",
        "  \n",
        "  Step 3: divide each by sum → [7.39/11.22, 2.72/11.22, 1.11/11.22]\n",
        "- multi_class='multinomial' uses Softmax regression\n",
        "- Model outputs probabilities for each class\n",
        "- Final prediction is class with highest probability\n",
        "\n",
        "###The confusion matrix for 3 classes shows:\n",
        "- Diagonal: Correct predictions for each class\n",
        "- Off-diagonal: Misclassifications between classes\n"
      ],
      "metadata": {
        "id": "9DIdy5pig0YQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Task: Demonstrate the importance of feature scaling for logistic regression."
      ],
      "metadata": {
        "id": "BMXovUAkg0Ou"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "import numpy as np\n",
        "\n",
        "# Create sample data with different scales\n",
        "np.random.seed(42)\n",
        "X_uneven = np.column_stack([\n",
        "    np.random.normal(100, 10, 100),  # Large scale (mean=100)\n",
        "    np.random.normal(0, 1, 100)      # Small scale (mean=0)\n",
        "])\n",
        "y_uneven = (X_uneven[:, 0] + X_uneven[:, 1] > 100).astype(int)\n",
        "\n",
        "# Split data\n",
        "X_train_u, X_test_u, y_train_u, y_test_u = train_test_split(\n",
        "    X_uneven, y_uneven, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Model without scaling\n",
        "model_no_scale = LogisticRegression(random_state=42)\n",
        "model_no_scale.fit(X_train_u, y_train_u)\n",
        "accuracy_no_scale = accuracy_score(y_test_u, model_no_scale.predict(X_test_u))\n",
        "\n",
        "# Model with scaling\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train_u)\n",
        "X_test_scaled = scaler.transform(X_test_u)\n",
        "\n",
        "model_scaled = LogisticRegression(random_state=42)\n",
        "model_scaled.fit(X_train_scaled, y_train_u)\n",
        "accuracy_scaled = accuracy_score(y_test_u, model_scaled.predict(X_test_scaled))\n",
        "\n",
        "print(f\"Accuracy without scaling: {accuracy_no_scale:.4f}\")\n",
        "print(f\"Accuracy with scaling: {accuracy_scaled:.4f}\")\n",
        "print(f\"Improvement: {accuracy_scaled - accuracy_no_scale:.4f}\")"
      ],
      "metadata": {
        "id": "TjHUrGoRg1Jq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "aebc2ad6-6a5c-4669-c6e5-e60c2af5fb55"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy without scaling: 0.9667\n",
            "Accuracy with scaling: 1.0000\n",
            "Improvement: 0.0333\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Why Scaling Matters:\n",
        "1. Logistic regression uses gradient descent\n",
        "2. Features with larger scales dominate the optimization\n",
        "3. Scaling ensures all features contribute equally\n",
        "4. StandardScaler transforms data to mean=0, std=1\n",
        "\n",
        "StandardScaler formula:\n",
        "    z = (x - mean) / std\n",
        "\n",
        "Always:\n",
        "\n",
        "- fit_transform on training data\n",
        "- transform on test data (using training parameters)\n"
      ],
      "metadata": {
        "id": "bGyMBb4whX-f"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Task: Create a complete pipeline for the breast cancer dataset with proper preprocessing."
      ],
      "metadata": {
        "id": "HZlB3oKm8Sdn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "# Create a pipeline that includes scaling and logistic regression\n",
        "pipeline = Pipeline([\n",
        "    ('scaler', StandardScaler()),           # Step 1: Scale features\n",
        "    ('classifier', LogisticRegression(      # Step 2: Classification\n",
        "        random_state=42,\n",
        "        max_iter=1000  # Ensure convergence\n",
        "    ))\n",
        "])\n",
        "\n",
        "# Use the pipeline on breast cancer data\n",
        "pipeline.fit(X_train, y_train)\n",
        "y_pred_pipeline = pipeline.predict(X_test)\n",
        "\n",
        "# Evaluate\n",
        "accuracy_pipeline = accuracy_score(y_test, y_pred_pipeline)\n",
        "cm_pipeline = confusion_matrix(y_test, y_pred_pipeline)\n",
        "\n",
        "print(f\"Pipeline Accuracy: {accuracy_pipeline:.4f}\")\n",
        "print(\"Pipeline Confusion Matrix:\")\n",
        "print(cm_pipeline)\n",
        "\n",
        "# Compare with original model\n",
        "print(f\"\\nImprovement over non-scaled: {accuracy_pipeline - accuracy:.4f}\")"
      ],
      "metadata": {
        "id": "jKet2g5t8RVc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}